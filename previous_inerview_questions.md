1. **I am working with two PySpark files where *File-A* creates a SparkSession. How can I import or reference this existing SparkSession in *File-B* to avoid creating duplicate sessions and ensure I'm working with the same Spark context?**

2. **Given a poorly performing SQL statement, how would you approach its optimization? Consider aspects such as query execution plans, indexing strategies, join operations, and query rewriting techniques in your answer.**

3. **You need to build an incremental data pipeline using PySpark that captures inserts, updates, and deletes from a source system. How would you implement CDC (Change Data Capture) logic to identify changes and synchronize them with the target data system while maintaining data consistency?**

4. **Describe a robust solution for schema evolution in PySpark streaming that addresses: automatic schema detection and inference, schema validation and compatibility checks, handling of schema mismatches, versioning strategies, and how to store evolving schemas alongside the data.**
